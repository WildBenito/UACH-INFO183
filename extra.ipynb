{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Make fonts readable at 1024x768 -->\n",
    "<style>\n",
    ".rendered_html { font-size:1.0em; }\n",
    ".container {width: 100%;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation, patches\n",
    "from IPython.display import display, Audio, HTML\n",
    "import soundfile as sf\n",
    "from style import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Bibliografía\n",
    "\n",
    "1. Weifeng Liu, José Príncipe, Simon Haykin, \"Kernel adaptive filtering\", *Wiley*\n",
    "1. Christopher M. Bishop, \"Pattern Recognition and Machine Learning\", *Springer*, **Capítulo 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "# Filtros adaptivos no-lineales \n",
    "***\n",
    "- Los filtros lineales son sencillos (baja capacidad)\n",
    "- Hasta ahora hemos asumido que la salida y la entrada se relacionan linealmente\n",
    "$$\n",
    "y_n = \\langle \\textbf{w}, \\textbf{u}_n \\rangle = \\textbf{w}^T \\textbf{u}_n = \\sum_{k=0}^L w_{k} u_{nk}\n",
    "$$\n",
    "- Si esto no se cumple el desempeño del filtro sería subóptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "fig, ax = plt.subplots(1, figsize=(9, 4))\n",
    "N = 1000;  L = 50\n",
    "u = np.linspace(0, 10, num=N)\n",
    "U = as_strided(u, [len(u)-L , L], strides=[u.strides[0], u.strides[0]])\n",
    "d = np.cos(2*np.pi*u) + 0.1*u\n",
    "Ruu = np.dot(U[:, :L].T, U[:, :L])\n",
    "Rud = np.dot(U[:, :L].T, d[L:, np.newaxis])\n",
    "h = np.linalg.solve(Ruu, Rud)[:, 0]\n",
    "ax.plot(u, d, lw=4, alpha=0.7, label='true')\n",
    "ax.plot(u[L:], np.dot(U, h), lw=4, alpha=0.7, label='estimated'); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Para solucionar esto necesitamos un modelo que no esté limitado a relaciones lineales\n",
    "$$\n",
    "y_n = f_\\Omega (\\textbf{u}_n)\n",
    "$$\n",
    "- Dos maneras de lograr esto \n",
    "    - Filtros no-lineales de topología fija: Perceptrón multicapa\n",
    "    - Método no-paramétrico: **Kernels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Kernels\n",
    "\n",
    "\n",
    "- Los filtros basados en kernels operan en un espacio de hilbert con kernel reproductor (RKHS)\n",
    "- **Espacio de Hilbert:** Espacio infinito de funciones con producto interno (producto punto)\n",
    "$$\n",
    "\\langle f, g \\rangle = \\int_{-\\infty}^\\infty f(x) g(x) dx < \\infty\n",
    "$$\n",
    "donde $f, g \\in H$ son funciones de $\\mathbb{R}^N \\to \\mathbb{R}$s donde $x\\in \\mathbb{R}^N$\n",
    "- **Propiedad reproductora:** Existe un elemento de $H$ llamado $\\varphi(x)$ tal que\n",
    "$$\n",
    "f(x) = \\langle f, \\varphi(x)  \\rangle \\quad \\forall f \\in H\n",
    "$$\n",
    "y entonces\n",
    "$$\n",
    "\\kappa(x, y) = \\langle \\varphi(x), \\varphi(y)  \\rangle\n",
    "$$\n",
    "se conoce como el **kernel reproductor** de $H$ a $\\kappa(x, y): \\mathbb{R}^N \\times \\mathbb{R}^N \\to \\mathbb{R}$ que es una función simétrica y semidefinido positiva\n",
    "\n",
    "Esta última igualdad se conoce como **truco del kernel**\n",
    "\n",
    "***\n",
    "Usando el truco del kernel podemos convertir un algoritmo lineal a no-lineal reeplazando un producto escalar por un kernel\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernels y transformaciones no lineales\n",
    "\n",
    "- Un kernel induce una transformación no lineal en la entrada\n",
    "- Por ejemplo consideremos el kernel polinomial de segundo grado y asumamos datos de entrada bidimensionales\n",
    "$$\n",
    "\\begin{align}\n",
    "\\kappa(x, y) &= \\langle x, y \\rangle^2 \\nonumber \\\\\n",
    "&= (x_1 y_1)^2 + 2 x_1 y_1 x_2 y_2 + (x_2 y_2)^2 \\nonumber \\\\\n",
    "&= \\Bigg \\langle \\begin{pmatrix} x_1^2 \\\\ \\sqrt{2} x_1 x_2 \\\\ x_2^2 \\end{pmatrix} \\begin{pmatrix} y_1^2 \\\\ \\sqrt{2} y_1 y_2 \\\\ y_2^2 \\end{pmatrix} \\Bigg \\rangle \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "- Se induce una transformación no lineal a un espacio de tres dimensiones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, Y = make_circles(n_samples=100, noise=0.08, factor=0.5); X = 10*X\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.scatter(X[:, 0]**2, X[:, 1]**2, np.sqrt(2)*X[:,0]*X[:,1], c=Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consideremos ahora el kernel Gaussiano o RBF para datos N-dimensionales\n",
    "$$\n",
    "\\begin{align}\n",
    "\\kappa(x, y) &= e^{-\\gamma \\|x-y\\|^2} \\nonumber \\\\\n",
    "&= e^{-\\gamma \\|x\\|^2} e^{-\\gamma \\|y\\|^2} e^{2\\gamma \\langle x, y \\rangle} \\nonumber \\\\\n",
    "&= e^{-\\gamma \\|x\\|^2} e^{-\\gamma \\|y\\|^2} \\sum_{k=0}^\\infty \\frac{1}{k!} (2\\gamma \\langle x, y \\rangle)^k \\nonumber \\\\\n",
    "&= \\sum_{k=0}^\\infty \\Bigg \\langle \\sqrt{2} \\gamma \\frac{x}{\\sqrt[{2k}]{k!}} e^{-\\gamma k^{-1} \\|x\\|^2}  , \\sqrt{2} \\gamma \\frac{y}{\\sqrt[{2k}]{k!}} e^{-\\gamma k^{-1} \\|y\\|^2}  \\Bigg \\rangle^k \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "- Se induce una transformación no lineal a un espacio infinito dimensional!\n",
    "- El kernel Gaussiano crea un RKHS con [capacidad de aproximación universal](jmlr.csail.mit.edu/papers/volume7/micchelli06a/micchelli06a.pdf)\n",
    "- El parámetro $\\gamma = \\frac{1}{2\\sigma^2}$ se escoge validación cruzada (k-fold si hay pocos datos) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Otros kernel válidos\n",
    "\n",
    "- Kernel polinomial inhomogeneo: $(\\langle x, y \\rangle +1)^d$\n",
    "- Kernel Cauchy: $\\frac{1}{1 + \\gamma \\|x-y\\|^2}$\n",
    "- Kernel periódico: $e^{-2 \\sin^2(\\pi |x-y|/P)/\\sigma^2}$\n",
    "- [Muchos kernels](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/)\n",
    "\n",
    "\n",
    "Construcción de kernels: [The kernel cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "# Kernel Least Mean Square (KLMS)\n",
    "\n",
    "***\n",
    "Recordemos, en el algoritmo LMS teniamos un filtro lineal\n",
    "$$\n",
    "y_n = \\langle \\textbf{w} , \\textbf{u}_n  \\rangle\n",
    "$$\n",
    "y minimizamos el error cuadrático instantaneo (SGD)\n",
    "$$\n",
    "e_n^2 = (d_n - y_n)^2\n",
    "$$\n",
    "lo que resultaba en\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_{n+1} &= \\textbf{w}_{n} + 2 \\mu e_n \\textbf{u}_{n}\\nonumber \\\\\n",
    "&= \\textbf{w}_{n} + 2 \\mu (d_n -  \\textbf{w}_{n}^T \\textbf{u}_{n}) \\textbf{u}_{n}, \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "***\n",
    "Asumamos que\n",
    "- Transformamos los datos usando $\\varphi(\\cdot)$, una transformacion no lineal\n",
    "- Nuestro filtro es lineal en el espacio transformado\n",
    "$$\n",
    "y_n = \\langle \\Omega , \\varphi(\\textbf{u}_n)  \\rangle\n",
    "$$\n",
    "- Los parámetros iniciales son nulos $\\Omega_0 = 0$\n",
    "\n",
    "Luego podemos actualizar los parámetros de nuestro filtro\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Omega_{n+1} &= \\Omega_{n} + 2 \\mu e_n \\varphi(\\textbf{u}_n) \\nonumber \\\\\n",
    "&= 2\\mu \\sum_{k=0}^n e_k \\varphi(\\textbf{u}_k) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "por lo que nuestro filtro no lineal \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\Omega(\\textbf{u}_{n+1}) &= \\langle \\Omega_{n+1} , \\varphi(\\textbf{u}_{n+1})  \\rangle \\nonumber \\\\\n",
    "&= 2\\mu \\sum_{k=0}^n e_k \\langle \\varphi(\\textbf{u}_k), \\varphi(\\textbf{u}_{n+1}) \\rangle \\nonumber \\\\\n",
    "&= 2\\mu \\sum_{k=0}^n e_k \\kappa(\\textbf{u}_k, \\textbf{u}_{n+1}) \\nonumber \\\\\n",
    "&= \\sum_{k=0}^n \\alpha_k \\kappa(\\textbf{u}_k, \\textbf{u}_{n+1}) \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- No necesitamos conocer $\\Omega$ \n",
    "- **Topología creciente:** Se necesita guardar $(\\alpha_k, \\textbf{u}_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algoritmo KLMS\n",
    "\n",
    "- Definir $\\mu$ y el kernel (e.g. RBF con parámetro $\\gamma$)\n",
    "- Inicializar $\\alpha_0 = 0$, inicializar diccionario $\\mathcal{D} = \\{\\}$\n",
    "- Para $n=1, \\ldots, N$\n",
    "    1. Calcular salida del filtro\n",
    "    $$\n",
    "    y_{n} = \\sum_{k=0}^{n-1} \\alpha_k \\kappa(\\textbf{u}_k, \\textbf{u}_{n})\n",
    "    $$\n",
    "    1. Calcular nuevo alpha\n",
    "    $$\n",
    "    \\alpha_{n} = 2 \\mu (d_{n}- y_{n})\n",
    "    $$\n",
    "    1. Guardar elemento en el diccionario\n",
    "    $$\n",
    "    \\mathcal{D} = \\{\\mathcal{D}, (\\alpha_n, u_n)\\}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Doblamiento de frecuencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N = 1000;  \n",
    "t = np.linspace(0, 4, num=N)\n",
    "u = np.cos(2*np.pi*t)\n",
    "d_clean = 2*u**2 #+ 0.5*u**3\n",
    "fig, ax = plt.subplots(2, figsize=(9, 4))\n",
    "\n",
    "def update(mu, gamma, L, rseed):\n",
    "    alfa, D = list(), list()\n",
    "    w = np.zeros(shape=(L+1, ))\n",
    "    np.random.seed(rseed)\n",
    "    d = d_clean + 1*np.random.randn(len(t))\n",
    "    ax[0].cla(); ax[1].cla(); \n",
    "    y = np.zeros(shape=(len(u), 2))\n",
    "    alfa.append(2*mu*d[L+1])\n",
    "    D.append(u[:L+1])\n",
    "    # KLMS\n",
    "    for n in range(L+1, N):\n",
    "        y[n, 0] = np.sum(np.array(alfa)*np.exp(-gamma*np.sum((u[n-L-1:n] - np.array(D))**2, axis=1)))\n",
    "        if n < 500:\n",
    "            alfa.append(2*mu*(d[n+1]-y[n, 0]))\n",
    "            D.append(u[n-L-1:n])\n",
    "    # LMS\n",
    "    for n in range(L+1, N):\n",
    "        y[n, 1] = np.dot(w, u[n-L-1:n])\n",
    "        if n < 500:\n",
    "            norm = np.sum(u[n-L-1:n]**2) + 1e-6\n",
    "            w += 2*mu*(d[n+1] - y[n, 1])*u[n-L-1:n]/norm\n",
    "    \n",
    "    ax[0].plot(t, d, 'k.', alpha=0.5); ax[0].plot(t, u, 'r-', alpha=0.5, lw=4);\n",
    "    ax[0].plot(t, d_clean, 'g-', alpha=0.75, lw=4);  ax[0].plot(t, y, alpha=0.75, lw=4); \n",
    "    ax[1].plot((d_clean - y[:, 0])**2, label='KLMS', alpha=0.5)\n",
    "    ax[1].plot((d_clean - y[:, 1])**2, label='LMS', alpha=0.5); ax[1].legend()\n",
    "    \n",
    "interact(update, mu=SelectionSlider_nice(options=[0.01, 0.05, 0.1, 0.2, 0.5, 1.], value=0.1),\n",
    "         gamma=SelectionSlider_nice(options=[0.001, 0.01, 0.1, 0.5, 1, 2, 5, 100.], value=1.), \n",
    "         L=SelectionSlider_nice(options=[1, 5, 10, 20, 50], value=10),\n",
    "         rseed=IntSlider_nice());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Criterio de novedad [(Platt, 1991)](https://dl.acm.org/citation.cfm?id=110091)\n",
    "\n",
    "- Para evitar que $\\mathcal{D}$ crezca demasiado se puede imponer un criterio de *sparsity*\n",
    "- Por ejemplo, solo incorporar al diccionario una muestra que sea \"distinta\" de las anteriores y cuyo error de predicción sea grande\n",
    "    - Si $\\min_{k=1,\\ldots, n} \\|(u_{n+1} - u_{k})\\|^2 > \\delta_1$\n",
    "    - y además $e_{n+1} > \\delta_2$\n",
    "    - entonces se incorpora $u_{n+1}$ a $\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# la señal enviada original\n",
    "r, fs = sf.read(\"data/hola1.ogg\")\n",
    "# El sistema que introduce ecos, por ejemplo una sala\n",
    "h = np.concatenate(([1.0], np.zeros(10), [0.7], np.zeros(10), [0.5], \n",
    "                    np.zeros(10), [0.3], np.zeros(10), [0.1], np.zeros(10), [0.05]))\n",
    "# la señal enviada con eco\n",
    "rh = np.convolve(r, h)[:len(r)]\n",
    "# la señal recibida limpia \n",
    "s, fs = sf.read(\"data/hola2.ogg\")\n",
    "s = np.concatenate((s, np.zeros(shape=(len(r)-len(s)))))\n",
    "# la señal recibida + señal enviada con eco + ruido blanco\n",
    "srh = s + 0.3*rh + np.random.randn(len(s))*0.005\n",
    "Audio(srh, rate=int(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "L, mu, gamma = 10, 0.01, 5.\n",
    "d1, d2 = 5e-3, 1e-2\n",
    "fig, ax = plt.subplots(2, figsize=(9, 6))\n",
    "w = np.zeros(shape=(L, ))\n",
    "rhhat = np.zeros(shape=(len(srh), 2))\n",
    "alfa, D = list(), list()\n",
    "alfa.append(2*mu*srh[L])\n",
    "D.append(u[0:L])\n",
    "for n in range(L+1, len(srh)):\n",
    "    norm = np.sum(r[n-L:n]**2) + 1e-10\n",
    "    rhhat[n, 0] = np.dot(w, r[n-L:n])\n",
    "    rhhat[n, 1] = np.sum(np.array(alfa)*np.exp(-gamma*np.sum((r[n-L:n] - np.array(D))**2, axis=1)))\n",
    "    w += 2*mu*(srh[n] - rhhat[n, 0])*r[n-L:n]/(norm)\n",
    "    dist = np.sum((r[n-L:n] - np.array(D))**2, axis=1)\n",
    "    if np.amin(dist) > d1 and np.absolute(srh[n] - rhhat[n, 1]) > d2:\n",
    "        alfa.append(2*mu*(srh[n]-rhhat[n, 1]))\n",
    "        D.append(r[n-L:n])\n",
    "        \n",
    "print(\"Tamaño del diccionario: %d\" %(len(alfa)))\n",
    "\n",
    "ax[0].plot(srh, alpha=0.5, label='hola2+hola1');\n",
    "ax[0].plot(srh - rhhat[:, 1], alpha=0.75, label='error KLMS');\n",
    "ax[0].plot(s, alpha=0.75, label='hola2 puro');\n",
    "ax[0].legend()\n",
    "ax[1].plot((s - srh + rhhat[:, 0])**2, label='LMS')\n",
    "ax[1].plot((s - srh + rhhat[:, 1])**2, label='KLMS'), ax[1].legend()\n",
    "print(np.mean((s - srh + rhhat[:, 1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#LMS\n",
    "Audio(srh - rhhat[:, 0], rate=int(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# KLMS\n",
    "Audio(srh - rhhat[:, 1], rate=int(fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Más allá de KLMS\n",
    "\n",
    "- [KLMS cuantizado](https://www.ncbi.nlm.nih.gov/pubmed/24808453)\n",
    "- [KLMS con tamaño de kernel adaptivo](https://arxiv.org/abs/1401.5899)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "# Regresión ridge con kernel (KRR)\n",
    "***\n",
    "\n",
    "- El filtro de Wiener es la versión offline del algoritmo LMS\n",
    "- La solución de Wiener es equivalente a una regresión lineal\n",
    "- Consideremos que\n",
    "    - Transformamos los datos usando $\\varphi(\\cdot)$, una transformacion no lineal\n",
    "    - Nuestro filtro es lineal en el espacio transformado\n",
    "$$\n",
    "y_n = \\langle \\Omega , \\varphi(\\textbf{u}_n)  \\rangle\n",
    "$$\n",
    "- Asumiendo que tenemos $N$ tuplas $(d_n, \\varphi(\\textbf{u}_n))$\n",
    "- Escribimos la función de costo del regresor regularizado en forma matricial como\n",
    "$$\n",
    "\\mathcal{L}(\\Omega) = (\\textbf{d} - \\Phi_u \\Omega)^T (\\textbf{d} - \\Phi_u \\Omega) + \\lambda \\Omega^T \\Omega\n",
    "$$\n",
    "con $\\textbf{d} = (d_1, d_2, \\dots, d_N)$, y \n",
    "$$\n",
    "\\Phi_u = \\begin{pmatrix} \\varphi(\\textbf{u_1}) \\\\ \\varphi(\\textbf{u_2}) \\\\ \\vdots \\\\ \\varphi(\\textbf{u_N}) \\end{pmatrix} \\in \\mathbb{R}^{N \\times M}\n",
    "$$\n",
    "donde $M$ es la dimensión de la transformación no lineal\n",
    "- Derivando e igualando a cero se tiene la solución de mínimos cuadrados\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Omega &= (\\Phi_u^T \\Phi_u + \\lambda I_M)^{-1} \\Phi_u^T \\textbf{d} \\nonumber \\\\\n",
    "&= \\Phi_u^T (\\Phi_u \\Phi_u^T + \\lambda I_N)^{-1} \\textbf{d} \\nonumber \\\\\n",
    "&= \\Phi_u^T (K + \\lambda I_N)^{-1} \\textbf{d} \\nonumber \\\\\n",
    "&= \\Phi_u^T \\alpha \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N \\varphi(\\textbf{u}_i)^T \\alpha_i \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "donde llamamos \n",
    "$$\n",
    "K = \\Phi_u \\Phi_u^T = \n",
    "\\begin{pmatrix} \n",
    "\\kappa(u_1, u_1) &  \\kappa(u_1, u_2)&  \\ldots& \\kappa(u_1, u_N) \\\\ \n",
    "\\kappa(u_2, u_1) &  \\kappa(u_2, u_2) & \\ldots& \\kappa(u_2, u_N) \\\\ \n",
    "\\vdots &  \\vdots & \\ddots & \\vdots \\\\\n",
    "\\kappa(u_N, u_1) &  \\kappa(u_N, u_2)& \\ldots& \\kappa(u_N, u_N) \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{N\\times N}\n",
    "$$ la matriz Gram\n",
    "- Para un nuevo valor $x$ la salida del regresor es\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\Omega(x) &= \\langle \\Omega,  \\varphi(x) \\rangle \\nonumber \\\\\n",
    "&= \\Omega^T  \\varphi(x) \\nonumber \\\\\n",
    "&= \\textbf{d}^T (K + \\lambda I_N)^{-1}  \\Phi_u \\varphi(x) \\nonumber \\\\\n",
    "&= \\textbf{d}^T (K + \\lambda I_N)^{-1} \\begin{pmatrix} \\kappa(u_1, x) \\\\ \\kappa(u_2, x) \\\\ \\vdots \\\\ \\kappa(u_N, x)\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "que no depende de $\\varphi(x)$ sólo del kernel!\n",
    "- Los parámetros del kernel y $\\lambda$ pueden seleccionarse con validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "help(KernelRidge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "lc_data, period = np.loadtxt(\"data/rrl.dat\"), 0.511941\n",
    "mjd, mag, err = lc_data[:, :3].T\n",
    "phi = np.mod(mjd, period)/period\n",
    "I = np.argsort(phi)\n",
    "phi, mag, err = np.hstack(([phi[I], phi[I]+1])), np.hstack(([mag[I], mag[I]])), np.hstack(([err[I], err[I]]))\n",
    "fig, ax = plt.subplots(1, figsize=(8, 3))\n",
    "ax.errorbar(phi, mag, err, fmt='.', c='k')\n",
    "ax.invert_yaxis()\n",
    "phi_reg = np.linspace(0, 2, num=100)\n",
    "line_rbf = ax.plot(phi_reg, np.mean(mag)+np.zeros_like(phi_reg), lw=4, alpha=0.5, label='RBF')\n",
    "line_lin = ax.plot(phi_reg, np.mean(mag)+np.zeros_like(phi_reg), lw=4, alpha=0.5, label='Linear')\n",
    "ax.legend();\n",
    "\n",
    "def update(lamb, gamma):\n",
    "    reg = KernelRidge(kernel='rbf', alpha=lamb, gamma=gamma)\n",
    "    reg.fit(phi.reshape(-1, 1), (mag - np.mean(mag))/np.std(mag))\n",
    "    mag_regular = reg.predict(phi_reg.reshape(-1, 1))\n",
    "    line_rbf[0].set_ydata(np.mean(mag)+ np.std(mag)*mag_regular)\n",
    "\n",
    "    reg = KernelRidge(kernel='linear', alpha=lamb)\n",
    "    reg.fit(phi.reshape(-1, 1), (mag - np.mean(mag))/np.std(mag))\n",
    "    mag_regular = reg.predict(phi_reg.reshape(-1, 1))\n",
    "    line_lin[0].set_ydata(np.mean(mag)+ np.std(mag)*mag_regular)\n",
    "\n",
    "interact(update, lamb=FloatSlider_nice(min=1e-3, max=1e-1, step=0.01),\n",
    "         gamma=SelectionSlider_nice(options=[1e-1, .5, 1, 5, 1e+1], value=1.));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Más allá de KRR\n",
    "\n",
    "- La versión Bayesiana de KRR se conoce como [proceso Gaussiano](http://www.gaussianprocess.org/)\n",
    "- El GP entrega incerteza en cada punto del regresor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernels para clasificar: Support Vector Machine\n",
    "\n",
    "Recordemos, la SVM busca resolver el siguiente problema de optimización con restricciones\n",
    "$$\n",
    "\\min_w  \\frac{1}{2} w^T w  - \\sum_i \\alpha_i [ y_i (w^T x_i + b) -1]\n",
    "$$\n",
    "donde \n",
    "- $w$ y $b$ son los parámetros del hiperplano\n",
    "- $\\alpha_i>0$ son los multiplicadores de Lagrange\n",
    "- Se cuenta con $N$ tuplas $(x_i, y_i)$ para entrenar con $x_i \\in \\mathbb{R}^M$ y $y_i \\in \\{-1,1\\}$\n",
    "- Se llama **vectores de soporte** (SV) a las tuplas con $\\alpha_i \\neq 0$\n",
    "\n",
    "Derivando en función de $w$ y $b$ se encuentra la forma dual\n",
    "$$\n",
    "\\max_\\alpha \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i, j} \\alpha_i \\alpha_j y_i y_j \\langle x_i, x_j \\rangle\n",
    "$$\n",
    "sujeto a $\\sum_i \\alpha_i y_i = 0$\n",
    "\n",
    "Aparece un producto punto que puede reemplazarse por un kernel!\n",
    "\n",
    "### C-SVM con kernel\n",
    "\n",
    "Hay que resolver \n",
    "$$\n",
    "\\max_\\alpha \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i, j} \\alpha_i \\alpha_j y_i y_j \\kappa(x_i, x_j)\n",
    "$$\n",
    "sujeto a $\\sum_i \\alpha_i y_i = 0$ y $0<\\alpha_i< C/N$\n",
    "\n",
    "Y el hiperplano para una muestra nueva es \n",
    "$$\n",
    "h(x) = \\sum_i \\alpha_i y_i \\kappa(x_i, x) + b\n",
    "$$\n",
    "\n",
    "A diferencia de KRR, la SVM es una solución *sparse*, es decir no ocupa todos los ejemplos (solo los SV)\n",
    "\n",
    "### Parámetros\n",
    "\n",
    "- $C$ representa un trade-off entre complejidad y ejemplos mal clasificados\n",
    "    - $C$ pequeño: Hiperplano suave, pocos vectores de soporte, \n",
    "    - $C$ grande: Hiperplano complejo, muchos vectores de soporte\n",
    "    - $C \\to \\infty$: Caso perfectamente separable\n",
    "- $\\gamma$ controla la influencia de los SV en la clasificación de un punto cualquiera $x$\n",
    "    - $\\gamma$ muy pequeño: Hiperplano muy simple tiende a una linea\n",
    "    - $\\gamma$ pequeño: Vectores lejanos a $x$ pueden influyen en su clase \n",
    "    - $\\gamma$ grande: Sólo los vectores más cercanos a $x$ influyen en su clase. \n",
    "    - $\\gamma$ muy grande: Hiperplano muy complejo y sobreajustado\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "help(SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "X, Y = make_blobs(n_samples=1000, centers=[[1, 1], [-1, 1], [1, -1], [-1, -1]], cluster_std=0.5)\n",
    "Y[Y==2], Y[Y==3] = 1.0, 0.0\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.25, test_size=0.75)\n",
    "# Create meshgrid for the probability plots\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3.5))\n",
    "\n",
    "def update(C, gamma):\n",
    "    ax[0].cla(); ax[1].cla();\n",
    "    classifier = SVC(C=C, kernel='rbf', gamma=gamma, probability=True)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_pred = classifier.predict_proba(X_test)\n",
    "    Z = classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    cr = ax[0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "    ax[0].scatter(X_train[classifier.support_, 0], X_train[classifier.support_, 1], \n",
    "               c='r', linewidth=1, alpha=0.25, s=100)\n",
    "    ax[0].scatter(X[Y==0, 0], X[Y==0, 1], c='k', marker='x', \n",
    "               s=20, alpha=0.5, label='class 1')\n",
    "    ax[0].scatter(X[Y==1, 0], X[Y==1, 1], c='k', marker='o', \n",
    "               s=20, alpha=0.5, label='class 2')\n",
    "\n",
    "    fpr, tpr, th = roc_curve(Y_test, Y_pred[:, 1])\n",
    "    ax[1].plot(fpr, tpr, linewidth=4, alpha=0.5, label='Test')\n",
    "    print(\"Area under the ROC curve (test): %f\" %(auc(fpr, tpr)))\n",
    "    fpr, tpr, th = roc_curve(Y_train, classifier.predict_proba(X_train)[:, 1])\n",
    "    ax[1].plot(fpr, tpr, linewidth=4, alpha=0.5, label='Train')\n",
    "    print(\"Area under the ROC curve (train): %f\" %(auc(fpr, tpr)))\n",
    "    plt.legend(loc=4)\n",
    "    ax[1].set_xlabel('False Positive Rate')\n",
    "    ax[1].set_ylabel('True Positive Rate')\n",
    "\n",
    "    print(\"%d SVs for class 1\" % (classifier.n_support_[0]))\n",
    "    print(\"%d SVs for class 2\" % (classifier.n_support_[1]))\n",
    "    \n",
    "interact(update, C=FloatSlider_nice(min=1e-2, max=1e-1, value=1e-2, step=0.01),\n",
    "         gamma=SelectionSlider_nice(options=[1e-2, 1e-1, .5, 1, 5, 1e+1], value=1.));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Otros métodos lineales kernelizables\n",
    "\n",
    "- Principal component analysis\n",
    "- Recursive least squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
